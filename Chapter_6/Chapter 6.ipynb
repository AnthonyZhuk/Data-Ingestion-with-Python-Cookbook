{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d188e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/04 11:06:42 WARN Utils: Your hostname, DESKTOP-DVUDB98 resolves to a loopback address: 127.0.1.1; using 172.31.148.225 instead (on interface eth0)\n",
      "23/01/04 11:06:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/01/04 11:06:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"Applying Schemas\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39a9d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = [(\"3456\",\"Cristian\",\"Rayner\",30,\"M\"),\n",
    "            (\"3567\",\"Guto\",\"Flower\",35,\"M\"),\n",
    "            (\"9867\",\"Yasmin\",\"Novak\",23,\"F\"),\n",
    "            (\"3342\",\"Tayla\",\"Mejia\",45,\"F\"),\n",
    "            (\"8890\",\"Barbara\",\"Kumar\",20,\"F\")\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7b3eb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# StructType -> Inicia a estrutura do nosso dataframe\n",
    "# StructField -> Inicia a estrutura do nosso campo ou linha\n",
    "# StringType -> Define aquele dado como tipo String\n",
    "# IntegerType -> Define o dado como tipo Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76e486e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora vamos estruturar nosso schema\n",
    "schema = StructType([ \\\n",
    "    StructField(\"id\",StringType(),True), \\\n",
    "    StructField(\"name\",StringType(),True), \\\n",
    "    StructField(\"lastname\",StringType(),True), \\\n",
    "    StructField(\"age\", IntegerType(), True), \\\n",
    "    StructField(\"gender\", StringType(), True), \\\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b3b0bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data=my_data,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "978f2b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb0b6ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66864dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"structured_schema\") \\\n",
    "      .config(\"spark.executor.memory\", '3g') \\\n",
    "      .config(\"spark.executor.cores\", '1') \\\n",
    "      .config(\"spark.cores.max\", '1') \\\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c13a939",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([ \\\n",
    "    StructField(\"artist_name\",StringType(),True), \\\n",
    "    StructField(\"track_name\",StringType(),True), \\\n",
    "    StructField(\"track_id\",StringType(),True), \\\n",
    "    StructField(\"popularity\", IntegerType(), True), \\\n",
    "    StructField(\"popularity_2\", IntegerType(), False)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "380fc36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.options(header= 'True', sep=',') \\\n",
    "                       .schema(schema) \\\n",
    "                       .csv('spotify_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f02d3a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+----------+------------+\n",
      "|      artist_name|          track_name|            track_id|popularity|popularity_2|\n",
      "+-----------------+--------------------+--------------------+----------+------------+\n",
      "|       Juice WRLD|All Girls Are The...|4VXIryQMWpIdGgYR4...|        83|        null|\n",
      "|Schoolgirl Byebye|           Year,2015|0UsmyJDsst2xhX1Zi...|        25|        null|\n",
      "|       Juice WRLD|        Lucid Dreams|285pBltuF7vW8TeWk...|        84|        null|\n",
      "|    Fleetwood Mac|Rhiannon (Will Yo...|4fbwTO3DJ2qryMddo...|        49|        null|\n",
      "|             Joji|SLOW DANCING IN T...|0rKtyWc8bvkriBthv...|        83|        null|\n",
      "|         Revenant|           Year 2018|7AFlWfLvu8ESnVrtH...|         0|        null|\n",
      "|    Morgan Wallen|     Whiskey Glasses|6foY66mWZN0pSRjZ4...|        78|        null|\n",
      "|    Frank Sinatra|It Was A Very Goo...|1vLPTWPfJSIrqOhNU...|        43|        null|\n",
      "|         Lil Baby|Drip Too Hard (Li...|78QR3Wp35dqAhFEc2...|        80|        null|\n",
      "|    Fleetwood Mac|Gypsy - 2018 Rema...|5nTnApD6zNvuHJe0f...|        46|        null|\n",
      "|    Billie Eilish|lovely (with Khalid)|0u2P5u6lvoDfwTYjA...|        87|        null|\n",
      "|    Anthem Lights|K-LOVE Fan Awards...|5rJw9VsPNdfnV9Ar9...|        31|        null|\n",
      "|      girl in red|we fell in love i...|1BYZxKSf0aTxp8ZFo...|        83|        null|\n",
      "|    Fleetwood Mac|Tusk - 2018 Remaster|6U0NPtExZu3tKg8ZD...|        48|        null|\n",
      "|     XXXTENTACION|                SAD!|3ee8Jmje8o58CHK66...|        84|        null|\n",
      "|    Anthem Lights|K-Love Fan Awards...|00ohIpPn9LkKpeIqh...|        26|        null|\n",
      "|       Luke Combs|     Beautiful Crazy|2rxQMGVafnNaRaXlR...|        78|        null|\n",
      "|    Fleetwood Mac|Dreams - 2018 Rem...|4pbO4YIjnPtGJce4q...|        48|        null|\n",
      "|         Lil Baby|          Yes Indeed|6vN77lE9LK6HP2Dew...|        78|        null|\n",
      "|    The Kiboomers|The Months of the...|58lQgf1Y5gRJRr6S0...|        28|        null|\n",
      "+-----------------+--------------------+--------------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/30 21:57:30 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 4, schema size: 5\n",
      "CSV file: file:///home/glauesppen/spotify_data.csv\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f5e159be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.options(header= 'True', \n",
    "                       sep=',', \n",
    "                       inferSchema='True') \\\n",
    "                .csv('spotify_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "420424e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- track_id: string (nullable = true)\n",
      " |-- popularity: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d854648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DoubleType, DateType\n",
    "schema = StructType([ \\\n",
    "    StructField(\"id\",IntegerType(),True), \\\n",
    "    StructField(\"name\",StringType(),True), \\\n",
    "    StructField(\"host_id\",IntegerType(),True), \\\n",
    "    StructField(\"host_name\",StringType(),True), \\\n",
    "    StructField(\"neighbourhood_group\",StringType(),True), \\\n",
    "    StructField(\"neighbourhood\",StringType(),True), \\\n",
    "    StructField(\"latitude\",DoubleType(),True), \\\n",
    "    StructField(\"longitude\",DoubleType(),True), \\\n",
    "    StructField(\"room_type\",StringType(),True), \\\n",
    "    StructField(\"price\",FloatType(),True), \\\n",
    "    StructField(\"minimum_nights\",IntegerType(),True), \\\n",
    "    StructField(\"number_of_reviews\",IntegerType(),True), \\\n",
    "    StructField(\"last_review\",DateType(),True), \\\n",
    "    StructField(\"reviews_per_month\",FloatType(),True), \\\n",
    "    StructField(\"calculated_host_listings_count\",IntegerType(),True), \\\n",
    "    StructField(\"availability_365\",IntegerType(),True), \\\n",
    "    StructField(\"number_of_reviews_ltm\",IntegerType(),True), \\\n",
    "    StructField(\"license\",StringType(),True)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e805baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/04 12:14:30 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.\n"
     ]
    }
   ],
   "source": [
    "df_1 = spark.read.options(header=True, sep=',', \n",
    "                          multiLine=True, escape='\"')\\\n",
    "                .schema(schema) \\\n",
    "                .csv('listings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "acd69c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- host_id: integer (nullable = true)\n",
      " |-- host_name: string (nullable = true)\n",
      " |-- neighbourhood_group: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- minimum_nights: integer (nullable = true)\n",
      " |-- number_of_reviews: integer (nullable = true)\n",
      " |-- last_review: date (nullable = true)\n",
      " |-- reviews_per_month: float (nullable = true)\n",
      " |-- calculated_host_listings_count: integer (nullable = true)\n",
      " |-- availability_365: integer (nullable = true)\n",
      " |-- number_of_reviews_ltm: integer (nullable = true)\n",
      " |-- license: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48a74dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/04 12:17:02 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.\n",
      "23/01/04 12:17:02 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 307.6 KiB, free 365.6 MiB)\n",
      "23/01/04 12:17:02 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 365.6 MiB)\n",
      "23/01/04 12:17:02 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.31.148.225:46585 (size: 27.6 KiB, free: 366.2 MiB)\n",
      "23/01/04 12:17:02 INFO SparkContext: Created broadcast 14 from csv at NativeMethodAccessorImpl.java:0\n",
      "23/01/04 12:17:02 INFO FileInputFormat: Total input files to process : 1\n",
      "23/01/04 12:17:02 INFO FileInputFormat: Total input files to process : 1\n",
      "23/01/04 12:17:02 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "23/01/04 12:17:02 INFO DAGScheduler: Got job 9 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/01/04 12:17:02 INFO DAGScheduler: Final stage: ResultStage 9 (csv at NativeMethodAccessorImpl.java:0)\n",
      "23/01/04 12:17:02 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/01/04 12:17:02 INFO DAGScheduler: Missing parents: List()\n",
      "23/01/04 12:17:02 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[27] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/01/04 12:17:02 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 6.4 KiB, free 365.6 MiB)\n",
      "23/01/04 12:17:02 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.7 KiB, free 365.6 MiB)\n",
      "23/01/04 12:17:02 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.31.148.225:46585 (size: 3.7 KiB, free: 366.2 MiB)\n",
      "23/01/04 12:17:02 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1388\n",
      "23/01/04 12:17:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[27] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/01/04 12:17:02 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
      "23/01/04 12:17:02 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (172.31.148.225, executor driver, partition 0, PROCESS_LOCAL, 4579 bytes) taskResourceAssignments Map()\n",
      "23/01/04 12:17:02 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)\n",
      "23/01/04 12:17:02 INFO BinaryFileRDD: Input split: Paths:/home/glauesppen/listings.csv:0+965593\n",
      "23/01/04 12:17:02 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 1239 bytes result sent to driver\n",
      "23/01/04 12:17:02 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 10 ms on 172.31.148.225 (executor driver) (1/1)\n",
      "23/01/04 12:17:02 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "23/01/04 12:17:02 INFO DAGScheduler: ResultStage 9 (csv at NativeMethodAccessorImpl.java:0) finished in 0.013 s\n",
      "23/01/04 12:17:02 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/01/04 12:17:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
      "23/01/04 12:17:02 INFO DAGScheduler: Job 9 finished: csv at NativeMethodAccessorImpl.java:0, took 0.015138 s\n",
      "23/01/04 12:17:02 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "23/01/04 12:17:02 INFO DAGScheduler: Got job 10 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/01/04 12:17:02 INFO DAGScheduler: Final stage: ResultStage 10 (csv at NativeMethodAccessorImpl.java:0)\n",
      "23/01/04 12:17:02 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/01/04 12:17:02 INFO DAGScheduler: Missing parents: List()\n",
      "23/01/04 12:17:02 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[28] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/01/04 12:17:02 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 9.0 KiB, free 365.6 MiB)\n",
      "23/01/04 12:17:02 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 365.6 MiB)\n",
      "23/01/04 12:17:02 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.31.148.225:46585 (size: 4.8 KiB, free: 366.2 MiB)\n",
      "23/01/04 12:17:02 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1388\n",
      "23/01/04 12:17:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[28] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/01/04 12:17:02 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
      "23/01/04 12:17:02 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (172.31.148.225, executor driver, partition 0, PROCESS_LOCAL, 4579 bytes) taskResourceAssignments Map()\n",
      "23/01/04 12:17:02 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)\n",
      "23/01/04 12:17:02 INFO BinaryFileRDD: Input split: Paths:/home/glauesppen/listings.csv:0+965593\n",
      "23/01/04 12:17:02 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 1282 bytes result sent to driver\n",
      "23/01/04 12:17:02 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 16 ms on 172.31.148.225 (executor driver) (1/1)\n",
      "23/01/04 12:17:02 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "23/01/04 12:17:02 INFO DAGScheduler: ResultStage 10 (csv at NativeMethodAccessorImpl.java:0) finished in 0.019 s\n",
      "23/01/04 12:17:02 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/01/04 12:17:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
      "23/01/04 12:17:02 INFO DAGScheduler: Job 10 finished: csv at NativeMethodAccessorImpl.java:0, took 0.020996 s\n"
     ]
    }
   ],
   "source": [
    "df_2 = spark.read.options(header=True, sep=',', \n",
    "                          multiLine=True, escape='\"',\n",
    "                         inferSchema=True) \\\n",
    "                .csv('listings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b38d3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- host_id: integer (nullable = true)\n",
      " |-- host_name: string (nullable = true)\n",
      " |-- neighbourhood_group: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- minimum_nights: integer (nullable = true)\n",
      " |-- number_of_reviews: integer (nullable = true)\n",
      " |-- last_review: string (nullable = true)\n",
      " |-- reviews_per_month: double (nullable = true)\n",
      " |-- calculated_host_listings_count: integer (nullable = true)\n",
      " |-- availability_365: integer (nullable = true)\n",
      " |-- number_of_reviews_ltm: integer (nullable = true)\n",
      " |-- license: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/04 12:36:44 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.31.148.225:46585 in memory (size: 27.6 KiB, free: 366.3 MiB)\n",
      "23/01/04 12:36:44 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 172.31.148.225:46585 in memory (size: 4.8 KiB, free: 366.3 MiB)\n",
      "23/01/04 12:36:44 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.31.148.225:46585 in memory (size: 9.2 KiB, free: 366.3 MiB)\n",
      "23/01/04 12:36:44 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.31.148.225:46585 in memory (size: 3.7 KiB, free: 366.3 MiB)\n",
      "23/01/04 12:36:44 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.31.148.225:46585 in memory (size: 27.5 KiB, free: 366.3 MiB)\n"
     ]
    }
   ],
   "source": [
    "df_2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5e8464c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = spark.read.option(\"multiline\",\"true\") \\\n",
    "                    .json('holiday_brazil.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f4cb0c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- holidays: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- country: string (nullable = true)\n",
      " |    |    |-- date: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- observed: string (nullable = true)\n",
      " |    |    |-- public: boolean (nullable = true)\n",
      " |    |    |-- uuid: string (nullable = true)\n",
      " |    |    |-- weekday: struct (nullable = true)\n",
      " |    |    |    |-- date: struct (nullable = true)\n",
      " |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |-- numeric: string (nullable = true)\n",
      " |    |    |    |-- observed: struct (nullable = true)\n",
      " |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |    |    |-- numeric: string (nullable = true)\n",
      " |-- requests: struct (nullable = true)\n",
      " |    |-- available: long (nullable = true)\n",
      " |    |-- resets: string (nullable = true)\n",
      " |    |-- used: long (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- warning: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "450999ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holidays</th>\n",
       "      <th>requests</th>\n",
       "      <th>status</th>\n",
       "      <th>warning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(BR, 2021-01-01, New Year's Day, 2021-01-01, ...</td>\n",
       "      <td>(9991, 2022-11-01 00:00:00, 9)</td>\n",
       "      <td>200</td>\n",
       "      <td>These results do not include state and provinc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            holidays  \\\n",
       "0  [(BR, 2021-01-01, New Year's Day, 2021-01-01, ...   \n",
       "\n",
       "                         requests  status  \\\n",
       "0  (9991, 2022-11-01 00:00:00, 9)     200   \n",
       "\n",
       "                                             warning  \n",
       "0  These results do not include state and provinc...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_json.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfbab913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/04 11:56:43 ERROR __main__: Error message sample\n",
      "23/01/04 11:56:43 INFO __main__: Info message sample\n"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "Logger= spark._jvm.org.apache.log4j.Logger\n",
    "syslogger = Logger.getLogger(__name__)\n",
    "syslogger.error(\"Error message sample\")\n",
    "syslogger.info(\"Info message sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a271c56",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/home/glauesppen/listings.cs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_340/1709506357.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m df = spark.read.options(header=True, sep=',', \n\u001b[0m\u001b[1;32m      2\u001b[0m                           \u001b[0mmultiLine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mescape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\"'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                          inferSchema=True) \\\n\u001b[1;32m      4\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'listings.cs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/home/glauesppen/listings.cs"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/01/04 12:06:44 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.31.148.225:46585 in memory (size: 4.8 KiB, free: 366.3 MiB)\n",
      "23/01/04 12:06:44 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.31.148.225:46585 in memory (size: 27.6 KiB, free: 366.3 MiB)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = spark.read.options(header=True, sep=',', \n",
    "                          multiLine=True, escape='\"',\n",
    "                         inferSchema=True) \\\n",
    "                .csv('listings.cs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aeac6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
