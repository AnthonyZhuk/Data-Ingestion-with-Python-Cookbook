{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ec0c7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9d8f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import logging\n",
    "from logging import config\n",
    "\n",
    "# Loading configuration file\n",
    "config.fileConfig(\"logging.conf\")\n",
    "\n",
    "# Creates a log configuration\n",
    "logger = logging.getLogger(\"data_ingest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7741e0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# import sys\n",
    "\n",
    "\n",
    "# def pysparkLogs(app_name):    \n",
    "#     logger = logging.getLogger(app_name)\n",
    "#     logger.setLevel(logging.INFO)\n",
    "#     formatter = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')\n",
    "\n",
    "#     stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "#     stdout_handler.setLevel(logging.DEBUG)\n",
    "#     stdout_handler.setFormatter(formatter)\n",
    "\n",
    "#     file_handler = logging.FileHandler('logs.log')\n",
    "#     file_handler.setLevel(logging.DEBUG)\n",
    "#     file_handler.setFormatter(formatter)\n",
    "\n",
    "\n",
    "#     logger.addHandler(file_handler)\n",
    "#     logger.addHandler(stdout_handler)\n",
    "    \n",
    "#     return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "002b1963",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/19 19:51:19 WARN Utils: Your hostname, EVA-01 resolves to a loopback address: 127.0.1.1; using 172.22.166.177 instead (on interface eth0)\n",
      "23/03/19 19:51:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/03/19 19:51:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"chapter8_monitoring\") \\\n",
    "      .config(\"spark.executor.memory\", '3g') \\\n",
    "      .config(\"spark.executor.cores\", '3') \\\n",
    "      .config(\"spark.cores.max\", '3') \\\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0161ac39",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3250/781538048.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_json\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_json' is not defined"
     ]
    }
   ],
   "source": [
    "df_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99afaa86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.22.163.5:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>chapter8_monitoring</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbb2b984b80>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c67e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n",
    "def _to_java_object_rdd(rdd):  \n",
    "    \"\"\" Return a JavaRDD of Object by unpickling\n",
    "    It will convert each Python object into Java object by Pyrolite, whenever the\n",
    "    RDD is serialized in batc h or not.\n",
    "    \"\"\"\n",
    "    rdd = rdd._reserialize(AutoBatchedSerializer(PickleSerializer()))\n",
    "    return rdd.ctx._jvm.org.apache.spark.mllib.api.python.SerDe.pythonToJava(rdd._jrdd, True)\n",
    "\n",
    "def estimate_df_size(df):\n",
    "    JavaObj = _to_java_object_rdd(df.rdd)\n",
    "    nbytes = spark._jvm.org.apache.spark.util.SizeEstimator.estimate(JavaObj)\n",
    "    return nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "096bd05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/19 19:51:25 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "df_json = spark.read.option(\"multiline\",\"true\") \\\n",
    "                    .json('github_events.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5eb9656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13029456"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_df_size(df_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4116ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_file_size(file_name, s_megabytes=False):\n",
    "    file_stats = os.stat(file_name)\n",
    "    if s_megabytes:\n",
    "        return f\"The file size in megabytes is: {file_stats.st_size / (1024 * 1024)}\"\n",
    "    return f\"The file size in bytes is: {file_stats.st_size}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f74a750",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stat: path should be string, bytes, os.PathLike or integer, not DataFrame",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_500/1508877918.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_file_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_500/2005806591.py\u001b[0m in \u001b[0;36mget_file_size\u001b[0;34m(file_name, s_megabytes)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_file_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_megabytes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfile_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ms_megabytes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34mf\"The file size in megabytes is: {file_stats.st_size / (1024 * 1024)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: stat: path should be string, bytes, os.PathLike or integer, not DataFrame"
     ]
    }
   ],
   "source": [
    "get_file_size(df_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a13dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
